{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83d9a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahak\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (C:\\Users\\tahak\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\inspection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partial_dependence, plot_partial_dependence\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (C:\\Users\\tahak\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\inspection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "sys.path.append('../src')\n",
    "from preprocess import HTRUPreprocessor\n",
    "from utils import VisualizationUtils\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "print(\"HTRU2 Pulsar Detection - Model Interpretability\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load best model and data\n",
    "best_model = joblib.load(\"../models/RandomForest_best.pkl\")['model']  # Update if different best model\n",
    "scaler = joblib.load(\"../models/scaler.pkl\")\n",
    "\n",
    "# Initialize preprocessor to get feature names\n",
    "preprocessor = HTRUPreprocessor()\n",
    "_, _, X_test, _, _, y_test = preprocessor.prepare_data()\n",
    "feature_names = preprocessor.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSHAP Value Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"SHAP Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/shap_summary.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Force plot for single observation\n",
    "print(\"\\nSHAP Force Plot for Single Prediction (Pulsar):\")\n",
    "sample_idx = np.where(y_test == 1)[0][0]  # First pulsar in test set\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][sample_idx,:], \n",
    "                X_test[sample_idx,:], feature_names=feature_names, matplotlib=True)\n",
    "plt.title(f\"SHAP Explanation for Pulsar Sample (True Class)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/shap_force_pulsar.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPartial Dependence Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get top 3 most important features\n",
    "importances = best_model.feature_importances_\n",
    "top_features = np.argsort(importances)[-3:][::-1]\n",
    "\n",
    "# Create PDP plots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, feature_idx in enumerate(top_features):\n",
    "    plot_partial_dependence(\n",
    "        best_model, X_test, [feature_idx], \n",
    "        feature_names=feature_names,\n",
    "        ax=ax[i]\n",
    "    )\n",
    "    ax[i].set_title(f\"PDP for {feature_names[feature_idx]}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/partial_dependence.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nError Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Identify misclassified samples\n",
    "fp_mask = (y_test == 0) & (y_pred == 1)  # False positives\n",
    "fn_mask = (y_test == 1) & (y_pred == 0)  # False negatives\n",
    "\n",
    "# Compare feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# False Positives vs True Negatives\n",
    "for i, feature in enumerate(feature_names[:4]):\n",
    "    sns.kdeplot(X_test[y_test==0, i], label=\"True Negatives\", ax=axes[0,0])\n",
    "    sns.kdeplot(X_test[fp_mask, i], label=\"False Positives\", ax=axes[0,0])\n",
    "axes[0,0].set_title(\"Feature Distributions: FP vs TN\")\n",
    "axes[0,0].legend()\n",
    "\n",
    "# False Negatives vs True Positives\n",
    "for i, feature in enumerate(feature_names[4:8]):\n",
    "    sns.kdeplot(X_test[y_test==1, 4+i], label=\"True Positives\", ax=axes[0,1])\n",
    "    sns.kdeplot(X_test[fn_mask, 4+i], label=\"False Negatives\", ax=axes[0,1])\n",
    "axes[0,1].set_title(\"Feature Distributions: FN vs TP\")\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Confidence scores\n",
    "sns.histplot(y_proba[y_test==0], label=\"Non-Pulsars\", kde=True, ax=axes[1,0])\n",
    "sns.histplot(y_proba[y_test==1], label=\"Pulsars\", kde=True, ax=axes[1,0])\n",
    "axes[1,0].axvline(0.5, color='red', linestyle='--')\n",
    "axes[1,0].set_title(\"Prediction Confidence Distribution\")\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Error cases by feature importance\n",
    "error_features = X_test[fp_mask | fn_mask][:, top_features]\n",
    "sns.boxplot(data=error_features, ax=axes[1,1])\n",
    "axes[1,1].set_xticks(range(len(top_features)))\n",
    "axes[1,1].set_xticklabels([feature_names[i] for i in top_features], rotation=45)\n",
    "axes[1,1].set_title(\"Top Features in Misclassified Samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/error_analysis.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDecision Threshold Optimization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Calculate precision-recall tradeoff\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Find optimal threshold (maximizing F1)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, precisions[:-1], label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], label=\"Recall\")\n",
    "plt.plot(thresholds, f1_scores[:-1], label=\"F1\")\n",
    "plt.axvline(optimal_threshold, color='red', linestyle='--')\n",
    "plt.xlabel(\"Decision Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall Tradeoff\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/threshold_optimization.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal Decision Threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"At this threshold:\")\n",
    "print(f\"- Precision: {precisions[optimal_idx]:.3f}\")\n",
    "print(f\"- Recall: {recalls[optimal_idx]:.3f}\")\n",
    "print(f\"- F1 Score: {f1_scores[optimal_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INTERPRETABILITY SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\"\"\n",
    "1. Key Findings:\n",
    "   - Most important features: {feature_names[top_features[0]]}, {feature_names[top_features[1]]}\n",
    "   - False positives tend to have higher values in profile statistics\n",
    "   - False negatives often have DM-SNR curve features similar to non-pulsars\n",
    "\n",
    "2. Model Behavior:\n",
    "   - Higher prediction confidence for true pulsars\n",
    "   - Decision boundary could be optimized (current threshold: 0.5)\n",
    "   - Suggested optimal threshold: {optimal_threshold:.2f}\n",
    "\n",
    "3. Recommendations:\n",
    "   - Consider feature engineering for problematic features\n",
    "   - Adjust decision threshold based on use case (precision vs recall)\n",
    "   - Monitor model performance on new data with similar characteristics\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
